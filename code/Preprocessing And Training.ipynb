{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 4 # batches to load data in\n",
    "RESIZE_TO = 400 # resize the image for training and transforms\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "CLASSES = ['background', 'without_mask', 'with_mask', 'mask_weared_incorrect']\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "IMG_DIR = '/content/drive/MyDrive/dataset/images'\n",
    "XML_DIR = '/content/drive/MyDrive/dataset/annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda (GPU) is available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset into train, validation, and train sets\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "def train_valid_test_split(img_dir=None, split=0.15):\n",
    "\n",
    "    # get all the img names\n",
    "    files = os.listdir(img_dir)\n",
    "\n",
    "    all_img = files\n",
    "\n",
    "    # shuffle the list\n",
    "    random.shuffle(all_img)\n",
    "\n",
    "    len_imgs = len(all_img)\n",
    "\n",
    "    # split into train/validation and test sets\n",
    "    trainTest_split = int((1-split)*len_imgs)\n",
    "\n",
    "    trainVal_df = all_img[:trainTest_split]\n",
    "    test_df = all_img[trainTest_split:]\n",
    "\n",
    "    # further split train/validation set into train and validation sets\n",
    "    lenTV_df = len(trainVal_df)\n",
    "\n",
    "    trainVal_split = int((1-split)*lenTV_df)\n",
    "\n",
    "    train_df = trainVal_df[:trainVal_split]\n",
    "    valid_df = trainVal_df[trainVal_split:]\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "train_set, valid_set, test_set = train_valid_test_split(img_dir = IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    " from albumentations.pytorch import ToTensorV2\n",
    "\n",
    " # define the training tranforms\n",
    " def get_train_transform():\n",
    "     return A.Compose([\n",
    "         A.Flip(0.5),\n",
    "         A.RandomRotate90(0.5),\n",
    "         A.MotionBlur(p=0.2),\n",
    "         A.MedianBlur(blur_limit=5, p=0.2),\n",
    "         A.Blur(blur_limit=5, p=0.2),\n",
    "         ToTensorV2(p=1.0),\n",
    "     ], bbox_params={\n",
    "         'format': 'pascal_voc',\n",
    "         'label_fields': ['labels']\n",
    "     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "class Averager:\n",
    "    \"\"\"\"\"\n",
    "    this class keeps track of the training and validation loss values...\n",
    "    and helps to get the average for each epoch as well\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number\n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "# Creating the dataset class\n",
    "\n",
    "class maskSet(Dataset):\n",
    "    def __init__(self, dataset, width, height, classes, img_dir_path, xml_dir_path, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.img_dir_path = img_dir_path\n",
    "        self.xml_dir_path = xml_dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.dataset = dataset\n",
    "        self.classes = classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.dataset[idx]\n",
    "        image_path = os.path.join(self.img_dir_path, image_name)\n",
    "\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # convert BGR to RGB color format and resize\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "\n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.xml_dir_path, annot_filename)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get the height and width of the image\n",
    "        for i in root.findall('size'):\n",
    "            image_width = int(i.find('width').text)\n",
    "            image_height = int(i.find('height').text)\n",
    "\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "\n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            ymax_final = (ymax/image_height)*self.height\n",
    "\n",
    "            # make sure bounding box doesn't exceed image dimensions\n",
    "            if xmin_final > self.width:\n",
    "                xmin_final = self.width\n",
    "\n",
    "            if ymin_final > self.height:\n",
    "                ymin_final = self.height\n",
    "\n",
    "            if xmax_final > self.width:\n",
    "                xmax_final = self.width\n",
    "\n",
    "            if ymax_final > self.height:\n",
    "                ymax_final = self.height\n",
    "\n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
    "\n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # crowd instances\n",
    "        if boxes.shape[0] > 1:\n",
    "            iscrowd = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        else:\n",
    "            iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # label to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "\n",
    "            target[\"boxes\"] = torch.Tensor(sample['bboxes'])\n",
    "\n",
    "            return image_resized, target\n",
    "\n",
    "        else:\n",
    "            image_resized = np.transpose(image_resized, (2, 0, 1))\n",
    "            image_resized = torch.from_numpy(image_resized)\n",
    "\n",
    "            return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = maskSet(train_set, RESIZE_TO, RESIZE_TO, CLASSES, IMG_DIR, XML_DIR)\n",
    "tran_trainDataset = maskSet(train_set, RESIZE_TO, RESIZE_TO, CLASSES, IMG_DIR, XML_DIR, get_train_transform())\n",
    "valid_dataset = maskSet(valid_set, RESIZE_TO, RESIZE_TO, CLASSES, IMG_DIR, XML_DIR)\n",
    "\n",
    "# concat original training data and tranformed data\n",
    "\n",
    "new_trainSet = ConcatDataset([train_dataset, tran_trainDataset])\n",
    "\n",
    "# defining train and validation sets data loaders\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    new_trainSet,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of training samples: {len(new_trainSet)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def create_model(num_classes):\n",
    "\n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "    # get the number of input features\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "\n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "\n",
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "\n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "SAVE_MODEL_EPOCH = 2 # save model after these many epochs\n",
    "NUM_EPOCHS = 20 # number of epochs to train for\n",
    "\n",
    "# initialize the model and move to the computation device\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "# train and validation loss lists to store loss values of all...\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    val_loss = validate(valid_loader, model)\n",
    "\n",
    "    print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n",
    "    print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")\n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n",
    "\n",
    "    if (epoch+1) == NUM_EPOCHS: # save model once at the end\n",
    "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/Kaggle/working/model{epoch+1}.pth\")\n",
    "\n",
    "    elif (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n",
    "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/Kaggle/working/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
