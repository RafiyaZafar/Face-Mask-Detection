{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b22de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    # get the number of input features\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3da97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 4\n",
    "DEVICE = 'cpu'\n",
    "CLASSES = ['background', 'without_mask', 'with_mask', 'mask_weared_incorrect']\n",
    "\n",
    "model_path = \"F:\\Sem-7\\Project\\Saved\\model18.pth\"\n",
    "\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dafc1",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6343212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # Calculate Intersection over Union (IoU) between two bounding boxes\n",
    "    \n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "#     print(\"iou\", iou)\n",
    "    return iou\n",
    "\n",
    "def calculate_ap(precision, recall):\n",
    "    # Calculate Average Precision (AP) using trapezoidal rule\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    indices = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = np.sum((mrec[indices + 1] - mrec[indices]) * mpre[indices + 1])\n",
    "    return ap\n",
    "\n",
    "def calculate_map(predictions, ground_truth, detection_threshold=0.5):\n",
    "    # predictions and ground_truth are lists of dictionaries, each containing 'box', 'class', and 'confidence'\n",
    "\n",
    "    classes = set([box['class'] for box in ground_truth])\n",
    "    num_classes = len(classes)\n",
    "#     print(\"num_classes\",len(classes))\n",
    "\n",
    "    total_ap = 0.0\n",
    "    total_iou = 0.0\n",
    "\n",
    "    for class_id in classes:\n",
    "        true_positives = np.zeros(len(predictions))\n",
    "        false_positives = np.zeros(len(predictions))\n",
    "        num_gt = 0\n",
    "        class_iou = 0.0\n",
    "\n",
    "        for i, pred_box in enumerate(predictions):\n",
    "#             print(pred_box, pred_box['class'], class_id)\n",
    "#             print(pred_box['class'] == class_id)\n",
    "            if pred_box['class'] == class_id and pred_box['confidence'] >= detection_threshold:\n",
    "                num_gt += 1\n",
    "                ious = [calculate_iou(pred_box['box'], gt_box['box']) for gt_box in ground_truth if gt_box['class'] == class_id]\n",
    "                \n",
    "                if any(iou >= 0.5 for iou in ious):\n",
    "                    true_positives[i] = 1\n",
    "                else:\n",
    "                    false_positives[i] = 1\n",
    "\n",
    "                class_iou += max(ious, default=0.0)\n",
    "\n",
    "#         print(true_positives)\n",
    "#         print(false_positives)\n",
    "#         print(num_gt)\n",
    "        cumulative_precision = np.cumsum(true_positives) / (np.cumsum(true_positives) + np.cumsum(false_positives))\n",
    "        cumulative_recall = np.cumsum(true_positives) / num_gt\n",
    "\n",
    "        ap = calculate_ap(cumulative_precision, cumulative_recall)\n",
    "\n",
    "        if not np.isnan(ap):\n",
    "            total_ap += ap\n",
    "        total_iou += class_iou / max(num_gt, 1)  # Avoid division by zero\n",
    "\n",
    "#     print(\"ap\",ap,\"total_ap\",total_ap)\n",
    "    mAP = total_ap / num_classes\n",
    "    mIoU = total_iou / num_classes\n",
    "    return mAP, mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0065d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(dataset):\n",
    "    total_mAP = 0.0\n",
    "    total_mIoU = 0.0\n",
    "    num_images = len(dataset)\n",
    "\n",
    "    for image_data in dataset:\n",
    "        # Extract predictions and ground truth for each image\n",
    "        predictions = image_data['predictions']\n",
    "        ground_truth = image_data['ground_truth']\n",
    "\n",
    "        # Call the calculate_map function for each image\n",
    "        mAP, mIoU = calculate_map(predictions, ground_truth, detection_threshold=0.5)\n",
    "        # print(mAP, mIoU)\n",
    "        # Accumulate mAP and mIoU for all images\n",
    "        total_mAP += mAP\n",
    "        total_mIoU += mIoU\n",
    "#         print(\"total_mAP\", total_mAP, \"total_mIoU\", total_mIoU)\n",
    "\n",
    "    print(\"num_images\", num_images)\n",
    "    # Calculate average mAP and mIoU across all images\n",
    "    average_mAP = total_mAP / num_images\n",
    "    average_mIoU = total_mIoU / num_images\n",
    "\n",
    "    return average_mAP, average_mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75057731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(images_dir,xml_dir, model, detection_threshold):\n",
    "\n",
    "    all_imgs = os.listdir(images_dir)\n",
    "    \n",
    "    split = 0.15\n",
    "    trainTest_split = int((1-split)*len(all_imgs))\n",
    "\n",
    "    trainVal_df = all_imgs[:trainTest_split]\n",
    "    test_imgs = all_imgs[trainTest_split:]\n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    width = 400\n",
    "    height = 400\n",
    "    \n",
    "    dataset = []\n",
    "    for img in test_imgs:\n",
    "        image_data = {}\n",
    "        # print(img)\n",
    "        predictions = []\n",
    "        \n",
    "        image = cv2.imread(images_dir+img)\n",
    "#         print(image.size)\n",
    "        orig_image = image.copy()\n",
    "        # BGR to RGB\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # make the pixel range between 0 and 1\n",
    "        image /= 255.0\n",
    "        # bring color channels to front\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(float)\n",
    "        # convert to tensor\n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        # add batch dimension\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image)\n",
    "\n",
    "        # load all detection to CPU for further operations\n",
    "        outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "#         print(outputs)\n",
    "        # carry further only if there are detected boxes\n",
    "        if len(outputs[0]['boxes']) != 0:\n",
    "            boxes = outputs[0]['boxes'].data.numpy()\n",
    "            scores = outputs[0]['scores'].data.numpy()\n",
    "            labels = outputs[0]['labels'].data.numpy()\n",
    "            \n",
    "            for box,label,score in zip(boxes,labels,scores):\n",
    "                t_dict = {}\n",
    "                t_dict['box'] = list(box)\n",
    "                t_dict['class'] = label\n",
    "                t_dict['confidence'] = score\n",
    "                predictions.append(t_dict)\n",
    "#         print(predictions)\n",
    "#         print()\n",
    "        \n",
    "        ground_truth = []\n",
    "        annot_img_name = img[:-4]+\".xml\"\n",
    "        annot_file_path = os.path.join(xml_dir, annot_img_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        classes_dict = {'without_mask': 1, 'with_mask': 2, 'mask_weared_incorrect': 3}\n",
    "\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get the height and width of the image\n",
    "        for i in root.findall('size'):\n",
    "            image_width = int(i.find('width').text)\n",
    "            image_height = int(i.find('height').text)\n",
    "\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(member.find('name').text)\n",
    "\n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        \n",
    "        for box,label in zip(boxes,labels):\n",
    "            t_dict = {}\n",
    "            t_dict['box'] = list(box)\n",
    "            t_dict['class'] = classes_dict[label]\n",
    "            ground_truth.append(t_dict)\n",
    "            \n",
    "        image_data['predictions'] = predictions\n",
    "        image_data['ground_truth'] = ground_truth\n",
    "        dataset.append(image_data)\n",
    "            \n",
    "#     print(dataset) \n",
    "    average_mAP, average_mIoU = evaluate_on_dataset(dataset)\n",
    "\n",
    "    print(f\"Average mAP: {average_mAP:.4f}, Average mIoU: {average_mIoU:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17abf79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22248\\3986820129.py:64: RuntimeWarning: invalid value encountered in divide\n",
      "  cumulative_precision = np.cumsum(true_positives) / (np.cumsum(true_positives) + np.cumsum(false_positives))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22248\\3986820129.py:65: RuntimeWarning: invalid value encountered in divide\n",
      "  cumulative_recall = np.cumsum(true_positives) / num_gt\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22248\\3986820129.py:65: RuntimeWarning: divide by zero encountered in divide\n",
      "  cumulative_recall = np.cumsum(true_positives) / num_gt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_images 128\n",
      "Average mAP: 0.8130, Average mIoU: 0.6980\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"F:/Sem-7/Project/dataset/images/\"\n",
    "xml_dir = \"F:/Sem-7/Project/dataset/annotations/\"\n",
    "detection_threshold = 0.8\n",
    "test_model(images_dir,xml_dir,model,detection_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2da03",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40206926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f4b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top=tk.Tk()\n",
    "top.geometry('800x600')\n",
    "top.title('Mask Detection')\n",
    "top.configure(background='#427D9D')\n",
    "\n",
    "label=Label(top,background='#CDCDCD', font=('arial',15,'bold'), text = \"check\")\n",
    "sign_image = Label(top)\n",
    "sign_image.pack(side=BOTTOM,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f466d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image():\n",
    "    try:\n",
    "        file_path=filedialog.askopenfilename()\n",
    "        uploaded=Image.open(file_path)\n",
    "        uploaded.thumbnail(((top.winfo_width()/2),(top.winfo_height()/2)))\n",
    "        im=ImageTk.PhotoImage(uploaded)\n",
    "        sign_image.configure(image=im)\n",
    "        sign_image.image=im\n",
    "        label.configure(text='')\n",
    "        show_classify_button(file_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def classify(file_path):\n",
    "    detection_threshold = 0.7\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    orig_image = image.copy()\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    # make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    # bring color channels to front\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(float)\n",
    "    # convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float)\n",
    "    # add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "#     print(outputs)\n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        labels = outputs[0]['labels'].data.numpy()\n",
    "#         print(\"labels before applyinng threshold\")\n",
    "#         print(labels)\n",
    "        # filter out boxes according to `detection_threshold`/\\\\\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        labels = labels[scores >= detection_threshold]\n",
    "#         print(\"labels after applyinng threshold\")\n",
    "#         print(labels)\n",
    "        draw_boxes = boxes.copy()\n",
    "\n",
    "        # draw the bounding boxes\n",
    "        for box, label in zip(draw_boxes, labels):\n",
    "            label = CLASSES[label]\n",
    "\n",
    "            if label == \"without_mask\":\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            elif label == \"with_mask\":\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            elif label == \"mask_weared_incorrect\":\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        color, 1)\n",
    "            cv2.putText(orig_image, label,\n",
    "                        (int(box[0]), int(box[1])-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "\n",
    "        image_rgb = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(image_rgb, 'RGB')\n",
    "        img.save(\"./check.png\")\n",
    "        uploaded=Image.open(\"./check.png\")\n",
    "        uploaded.thumbnail(((top.winfo_width()/2),(top.winfo_height()/2)))\n",
    "        im=ImageTk.PhotoImage(uploaded)\n",
    "        sign_image.configure(image=im)\n",
    "        sign_image.image=im\n",
    "\n",
    "\n",
    "\n",
    "def show_classify_button(file_path):\n",
    "    classify_b=Button(top,text=\"Classify Image\",command=lambda: classify(file_path),padx=10,pady=5)\n",
    "    classify_b.configure(background='#164863', foreground='white',font=('arial',10,'bold'))\n",
    "    classify_b.place(relx=0.79,rely=0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82237211",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload=Button(top,text=\"Upload an image\",command=upload_image,padx=10,pady=5)\n",
    "upload.configure(background='#164863', foreground='white',font=('arial',10,'bold'))\n",
    "upload.pack(side=BOTTOM,pady=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
